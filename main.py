from fastapi import FastAPI, HTTPException, Query
from pydantic import BaseModel
from typing import List, Optional
import sqlite3
import requests
from bs4 import BeautifulSoup
import re
from openai import OpenAI
import os
import time

from dotenv import load_dotenv

# Load .env file
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# import streamlit as st
# api_key = st.secrets["OPENAI_API_KEY"]

if not api_key:
    raise ValueError("âŒ OpenAI API Key not found, please check the .env file!")

client = OpenAI()

# Initialize FastAPI
app = FastAPI()

# Connect to SQLite database
DB_FILE = "links.db"

def init_db():
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS links (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE,
            type TEXT,
            relevance_score REAL,
            keywords TEXT
        )
    """)
    conn.commit()
    conn.close()

init_db()  # Initialize database

# Pydantic models
class ScrapeRequest(BaseModel):
    url: str
    use_gpt: bool = False  # Whether to use GPT
    keywords: List[str]  # List of keywords

class LinkResponse(BaseModel):
    url: str
    type: str
    relevance_score: float
    keywords: List[str]

def save_links(links):
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    for url, link_type, relevance_score, keywords in links:
        cursor.execute("""
            INSERT INTO links (url, type, relevance_score, keywords)
            VALUES (?, ?, ?, ?)
            ON CONFLICT(url) DO UPDATE SET
                type = excluded.type,
                relevance_score = excluded.relevance_score,
                keywords = excluded.keywords
        """, (url, link_type, relevance_score, keywords))
    conn.commit()
    conn.close()

def scrape_webpage(url):
    """Fetches and extracts readable text from a webpage."""
    try:
        headers = {"User-Agent": "Mozilla/5.0"}  # Avoid getting blocked
        response = requests.get(url, headers=headers, timeout=5)
        response.raise_for_status()  # Raise error for bad status codes

        soup = BeautifulSoup(response.text, "html.parser")

        # Remove unnecessary tags (script, style)
        for tag in soup(["script", "style"]):
            tag.extract()

        # Extract and clean text
        text = soup.get_text(separator=" ", strip=True)
        return text[:5000]  # Limit to 5000 chars (to avoid GPT token limits)

    except Exception as e:
        print(f"âš ï¸ Webpage scraping failed for {url}: {e}")
        return None  # Return None if scraping fails

def analyze_with_gpt(url, keywords):
    """Analyzes a URL's relevance to multiple keywords using GPT."""
    webpage_text = scrape_webpage(url)

    prompt = f"""Analyze the relevance of the following webpage to the given keywords: "{keywords}". 

If webpage content is available, use it to determine the main topic and its connection to each keyword. Otherwise, base the relevance on the URL structure.

For each keyword, return a relevance score between 0 and 1 based on the following guidelines:
- 0.0: Completely unrelated
- 0.2: Slightly related (keyword appears but seems incidental)
- 0.5: Somewhat relevant (keyword appears, but topic is broader)
- 0.8: Highly relevant (webpage strongly focuses on keyword)
- 1.0: Directly relevant (keyword is the main subject)

Return **only a JSON object** with keyword-score pairs. Example format:
{{
    "keyword1": 0.8,
    "keyword2": 0.3,
    "keyword3": 0.5
}}

URL: {url}
"""

    if webpage_text:
        prompt += f"\nWebpage Content:\n{webpage_text[:3000]}"

    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        answer = response.choices[0].message.content.strip()
        print(f"ðŸ“ GPT Output: {answer}")

        # è§£æž GPT è¿”å›žçš„ JSON æ ¼å¼ç»“æžœ
        import json
        scores = json.loads(answer)

        # è®¡ç®—ç½‘é¡µä¸Žå¤šä¸ªå…³é”®è¯çš„ç»¼åˆç›¸å…³åº¦
        avg_score = sum(scores.values()) / len(scores)  # è®¡ç®—å¹³å‡åˆ†
        max_score = max(scores.values())  # è®¡ç®—æœ€é«˜åˆ†

        return avg_score, max_score, scores  # è¿”å›žå¤šç§æŒ‡æ ‡ï¼Œæ–¹ä¾¿ä½¿ç”¨

    except Exception as e:
        print(f"âŒ GPT processing failed: {e}")
        return 0.3, 0.3, {}  # é»˜è®¤åˆ†æ•°

   
def scrape_links(url, use_gpt, keywords):
    print("Keywords:", keywords)
    """ Scrape all links from a webpage and filter high-value links """
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=400, detail=f"Failed to fetch URL: {str(e)}")

    soup = BeautifulSoup(response.text, "html.parser")
    links = soup.find_all("a", href=True)

    extracted_links = []
    total_links = len(links)
    for i, link in enumerate(links):
        href = link["href"]
        full_url = requests.compat.urljoin(url, href)
        
        # Match user-provided keywords
        keywords_found = [kw for kw in keywords if kw.lower() in href.lower()]

        relevance_score = 1.0 if keywords_found else 0.3

        # Identify type
        link_type = "document" if href.endswith((".pdf", ".xls", ".xlsx", ".doc", ".docx")) else "webpage"

        extracted_links.append((full_url, link_type, relevance_score, ",".join(keywords_found)))

        # print(f"âœ… Processed {i+1}/{total_links} links", ",".join(keywords_found), "score: ", relevance_score)
    
    return extracted_links

@app.post("/scrape")
def scrape(request: ScrapeRequest):
    links = scrape_links(request.url, request.use_gpt, request.keywords)
    save_links(links)
    return {"message": "Scraping completed", "scraped_links": len(links)}

@app.get("/links", response_model=List[LinkResponse])
def get_links(
    keyword: Optional[str] = Query(None),
    min_score: float = Query(0.0),
    use_gpt: bool = Query(False)
):
    
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    query = "SELECT url, type, relevance_score, keywords FROM links WHERE relevance_score >= ?"
    params = [min_score]

    if keyword:
        query += " AND keywords LIKE ?"
        params.append(f"%{keyword}%")

    cursor.execute(query, params)
    results = cursor.fetchall()
    conn.close()

    filtered_results = []
    for row in results:
        url, link_type, score, stored_keywords = row
        if not use_gpt: 
            filtered_results.append({
                "url": url,
                "type": link_type,
                "relevance_score": score,
                "keywords": stored_keywords.split(",")
            })
        else:
            gpt_score = analyze_with_gpt(url, stored_keywords)
            filtered_results.append({
                "url": url,
                "type": link_type,
                "relevance_score": gpt_score[0],
                "keywords": stored_keywords.split(",")
            })

    return filtered_results
